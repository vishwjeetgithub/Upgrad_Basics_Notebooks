{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Q0gtYZwQCXjA-9XIYn1bA-wFb2gISO3l","timestamp":1711027894618}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"eL5pzZIa5sEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711027943980,"user_tz":-330,"elapsed":18091,"user":{"displayName":"Henry Isaac","userId":"09127918064436985573"}},"outputId":"ebab62c1-a235-4735-da22-61da07300d3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -U -q openai"]},{"cell_type":"code","source":["import os\n","import openai\n","# Set the API key\n","with open(\"OPENAI_API_Key.txt\", \"r\") as f:\n","  openai.api_key = ' '.join(f.readlines())"],"metadata":{"id":"Qhn0yChQiazw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chat Completion Parameters"],"metadata":{"id":"uGZzXIDH7HX7"}},{"cell_type":"markdown","source":["You are already familiar with a few of the parameters such as `temperature`, `top_p`,`max_tokens`,`stop`, `frequency_penalty`,`presence_penalty` etc.\n","<br>\n","ChatCompletions API offers additional capabilities as described in its [API documentation](https://platform.openai.com/docs/api-reference/chat/create)\n","\n","In this notebook, we will explore a few additional parameters for the ChatCompletions API such as:\n","  * `logprobs`\n","  * `top_logprobs`\n","  * `seed`\n","\n","\n"],"metadata":{"id":"1e8xeCZr6akQ"}},{"cell_type":"markdown","source":["## Seed Parameter"],"metadata":{"id":"pFB02_dn7FR1"}},{"cell_type":"markdown","source":["The `seed` parameter allows you to acheive more consistent outputs. It helps you control the deterministic nature of the answers. In other words, making the same requests with the same value of `seed` parameter will return the same results, ensuring reproducibility.\n","\n","You can check the `system_fingerprint` value in the output to see the current  combination of model weights, infrastructure, configuration etc used by the servers.  The `seed` is stored with the prompt in order to replicate the response.\n","\n","\n","\n","**Value**: By setting the value of the `seed` parameter as an integer (e.g, 123), you can control the determinism in outputs.\n","\n","**Consistency in behaviour**: If you set the `seed` parameter to the same value across requests, while also keeping other parameters like `prompt`, `temperature`, and `top_p` consistent, the model likely produce identical outputs.\n","\n","\n","\n","\n","Please note that the `seed` feature is in beta phase and is currently supported only for `gpt-4-1106-preview` and `gpt-3.5-turbo-1106` models."],"metadata":{"id":"r27olwGSibl1"}},{"cell_type":"code","source":["def chat_response_nonseeded(system_message, user_request):\n","  messages = [\n","      {\"role\": \"system\", \"content\": system_message},\n","      {\"role\": \"user\", \"content\": user_request},\n","    ]\n","  response = openai.chat.completions.create(\n","      model = \"gpt-3.5-turbo-1106\",\n","      temperature = 0.5,\n","      messages=messages,\n","      max_tokens=200\n","    )\n","  return response.choices[0].message.content"],"metadata":{"id":"PZBcMVt76JI4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def chat_response_seeded(system_message, user_request):\n","  messages = [\n","      {\"role\": \"system\", \"content\": system_message},\n","      {\"role\": \"user\", \"content\": user_request},\n","    ]\n","  chat_response = openai.chat.completions.create(\n","      model = \"gpt-3.5-turbo-1106\",\n","      temperature = 0.7,\n","      messages=messages,\n","      seed=234,\n","      max_tokens=200,\n","    )\n","  return chat_response.choices[0].message.content"],"metadata":{"id":"ABstZo7cwfvJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","system_message = \"You are a chatbot that the user can discuss with. Answer user questions in a conversational way.\"\n","user_request = \"Output a fruit\"\n","non_seed_answer = []\n","seed_answer = []\n","i = 0\n","while (i < 5):\n","  print(f\"Loop {i}\")\n","  ans1 = chat_response_nonseeded(system_message, user_request)\n","  non_seed_answer.append(ans1)\n","  print(\"Non Seeded Answer:\",non_seed_answer[i])\n","  ans2 = chat_response_seeded(system_message, user_request)\n","  seed_answer.append(ans2)\n","  print(\"Seeded Answer:\",seed_answer[i])\n","  i+=1\n","  print('='*10)\n","  time.sleep(10)"],"metadata":{"id":"lVKUiGqnAc99","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711028651381,"user_tz":-330,"elapsed":54790,"user":{"displayName":"Henry Isaac","userId":"09127918064436985573"}},"outputId":"f5aea20e-5f88-47be-e4eb-a8a764a9281e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loop 0\n","Non Seeded Answer: How about an apple? It's a classic choice and comes in many delicious varieties!\n","Seeded Answer: How about a juicy and delicious apple?\n","==========\n","Loop 1\n","Non Seeded Answer: How about a juicy, ripe mango?\n","Seeded Answer: How about a juicy and delicious apple?\n","==========\n","Loop 2\n","Non Seeded Answer: How about a juicy, ripe mango?\n","Seeded Answer: How about a juicy and delicious apple?\n","==========\n","Loop 3\n","Non Seeded Answer: How about a juicy, ripe peach?\n","Seeded Answer: How about a juicy and delicious apple?\n","==========\n","Loop 4\n","Non Seeded Answer: Sure, how about a juicy and delicious apple?\n","Seeded Answer: How about a juicy and delicious apple?\n","==========\n"]}]},{"cell_type":"markdown","source":["An important thing to understand is the difference between `seed` and `temperature` parameters.\n","<br>\n","\n","*   While seed is used to control the *randomness* or *stochasticity* of the outputs, `temperature` is used to control the *creativity* or *variability* of generated text.\n","*   `seed` value ensures that there is a consistency in the outputs generated by requests with the same parameters, and `temperature` focusses more on randomness in selection of next tokens.\n","* `seed` is assigned an integer value, while `temperature` is assigned float values between 0 and 2\n","<br>\n","\n","However, you should remember that determinism cannot be always guaranteed due to the inherent non determinism in LLM models, but this approach will lead to somewhat consistent results. <br>"],"metadata":{"id":"BW2TRc79pLX5"}},{"cell_type":"markdown","source":["## Logprobs Parameter"],"metadata":{"id":"0ETU4OHCUKVG"}},{"cell_type":"markdown","source":["When logprobs is enabled, the API returns the log probabilities of each output token, along with a limited number of the most likely tokens at each token position and their log probabilities. The relevant request parameters are:\n","- `logprobs`: Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message. This option is currently not available on the gpt-4-vision-preview model.\n","- `top_logprobs`: An integer between 0 and 5 specifying the number of most likely tokens to return at each token position, each with an associated log probability. logprobs must be set to true if this parameter is used.\n","Log probabilities of output tokens indicate the likelihood of each token occurring in the sequence given the context. To simplify, a logprob is log(p), where p = probability of a token occurring at a specific position based on the previous tokens in the context. Some key points about logprobs:\n","\n","Higher log probabilities suggest a higher likelihood of the token in that context. This allows users to gauge the model's confidence in its output or explore alternative responses the model considered.\n","Logprob can be any negative number or 0.0. 0.0 corresponds to 100% probability.\n","\n","Logprobs allow us to compute the joint probability of a sequence as the sum of the logprobs of the individual tokens. This is useful for scoring and ranking model outputs. Another common approach is to take the average per-token logprob of a sentence to choose the best generation.\n","We can examine the logprobs assigned to different candidate tokens to understand what options the model considered plausible or implausible.\n"],"metadata":{"id":"rt_cI-OyUM7h"}},{"cell_type":"code","source":["from math import exp\n","import pandas as pd\n","import numpy as np\n","import os"],"metadata":{"id":"BPyT1n3lUMH0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def chat_response(system_message, user_request):\n","  messages = [\n","      {\"role\": \"system\", \"content\": system_message},\n","      {\"role\": \"user\", \"content\": user_request},\n","    ]\n","  response = openai.chat.completions.create(\n","      model = \"gpt-3.5-turbo-1106\",\n","      temperature = 0.5,\n","      messages=messages,\n","      max_tokens=200,\n","      logprobs=True,\n","      top_logprobs=2\n","    )\n","  return response\n","\n","SYS_MSG = 'You are a helpful AI Assistant'\n","USR_RQST = 'What is the capital of France?'\n","response = chat_response(SYS_MSG, USR_RQST)\n","output_reponse = response.choices[0].message.content\n","top_two_logprobs = response.choices[0].logprobs.content[0].top_logprobs\n","print(output_reponse, top_two_logprobs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IFCqGsPORJMP","executionInfo":{"status":"ok","timestamp":1711029244131,"user_tz":-330,"elapsed":679,"user":{"displayName":"Henry Isaac","userId":"09127918064436985573"}},"outputId":"b106a4cd-b6b5-4835-d3c6-20b8ef403d4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The capital of France is Paris. [TopLogprob(token='The', bytes=[84, 104, 101], logprob=-0.0002291655), TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-8.99922)]\n"]}]},{"cell_type":"code","source":["response #Detailed ChatCompletion Object"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TdyoIe74So-T","executionInfo":{"status":"ok","timestamp":1711029263856,"user_tz":-330,"elapsed":3,"user":{"displayName":"Henry Isaac","userId":"09127918064436985573"}},"outputId":"41724248-01f1-4877-d36b-ee96e7173bb2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ChatCompletion(id='chatcmpl-95D9lnPph8N5H1Pm5gjt5ITH5NLl3', choices=[Choice(finish_reason='stop', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='The', bytes=[84, 104, 101], logprob=-0.0002291655, top_logprobs=[TopLogprob(token='The', bytes=[84, 104, 101], logprob=-0.0002291655), TopLogprob(token='Paris', bytes=[80, 97, 114, 105, 115], logprob=-8.99922)]), ChatCompletionTokenLogprob(token=' capital', bytes=[32, 99, 97, 112, 105, 116, 97, 108], logprob=-3.0545007e-06, top_logprobs=[TopLogprob(token=' capital', bytes=[32, 99, 97, 112, 105, 116, 97, 108], logprob=-3.0545007e-06), TopLogprob(token=' ', bytes=[32], logprob=-13.492818)]), ChatCompletionTokenLogprob(token=' of', bytes=[32, 111, 102], logprob=-0.00036822853, top_logprobs=[TopLogprob(token=' of', bytes=[32, 111, 102], logprob=-0.00036822853), TopLogprob(token=' city', bytes=[32, 99, 105, 116, 121], logprob=-7.908575)]), ChatCompletionTokenLogprob(token=' France', bytes=[32, 70, 114, 97, 110, 99, 101], logprob=-9.0883464e-07, top_logprobs=[TopLogprob(token=' France', bytes=[32, 70, 114, 97, 110, 99, 101], logprob=-9.0883464e-07), TopLogprob(token=' ', bytes=[32], logprob=-15.156457)]), ChatCompletionTokenLogprob(token=' is', bytes=[32, 105, 115], logprob=-9.0883464e-07, top_logprobs=[TopLogprob(token=' is', bytes=[32, 105, 115], logprob=-9.0883464e-07), TopLogprob(token=' ', bytes=[32], logprob=-14.90524)]), ChatCompletionTokenLogprob(token=' Paris', bytes=[32, 80, 97, 114, 105, 115], logprob=-1.6240566e-06, top_logprobs=[TopLogprob(token=' Paris', bytes=[32, 80, 97, 114, 105, 115], logprob=-1.6240566e-06), TopLogprob(token=' ', bytes=[32], logprob=-13.999834)]), ChatCompletionTokenLogprob(token='.', bytes=[46], logprob=-0.00016539889, top_logprobs=[TopLogprob(token='.', bytes=[46], logprob=-0.00016539889), TopLogprob(token=',', bytes=[44], logprob=-9.220153)])]), message=ChatCompletionMessage(content='The capital of France is Paris.', role='assistant', function_call=None, tool_calls=None))], created=1711029241, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_9dd82289bf', usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31))"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["Example 2"],"metadata":{"id":"Erg7pOFFUOnw"}},{"cell_type":"code","source":["LAPTOP_PROD_DESC = \"\"\"You will be given the laptop specifications and product description for a laptop.\n","Based on the description, classify the laptop into one of the following categories: general, business, student, gamer, content, programmer, multimedia.\n","Return only the name of the category, and nothing else.\n","MAKE SURE your output is one of the seven categories stated.\n","Laptop Description: {description}\"\"\""],"metadata":{"id":"X6KuIujeb4bi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["description_1 = \"\"\"\n","The HP EliteBook is a premium laptop designed for professionals who value performance and security.\n","It features an Intel Core i7 processor clocked at 2.8 GHz, providing exceptional speed and power for demanding tasks.\n","With 16GB of RAM and an SSD, it offers fast and efficient multitasking and storage capabilities.\n","The laptop sports a 14\" LED display with a resolution of 1920x1080, delivering sharp visuals and vibrant colors.\n","It also comes with an Intel UHD GPU for decent graphical performance.\n","Weighing just 1.5 kg, it is lightweight and highly portable.\n","The laptop is equipped with a fingerprint sensor for secure and convenient login.\n","With a three-year warranty and a battery life of up to 8 hours, it ensures long-lasting reliability.\n","Priced at 90,000, the HP EliteBook provides top-notch features and durability for professional users.\n","\"\"\""],"metadata":{"id":"b-DChw9kcjjb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SYSTEM_MSG = \"You are a helpful AI assistant that can evaluate laptop products based on their description.\"\n","USR_RQST = LAPTOP_PROD_DESC.format(description=description_1)\n","response = chat_response(SYS_MSG, USR_RQST)\n","response.choices[0].message.content"],"metadata":{"id":"_YQ5k-O7drMr","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1711029581969,"user_tz":-330,"elapsed":554,"user":{"displayName":"Henry Isaac","userId":"09127918064436985573"}},"outputId":"e65e84e4-17fe-4ae1-86de-d35dfb0f8bf5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'business'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["response.choices[0].logprobs.content[0].top_logprobs"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711029590955,"user_tz":-330,"elapsed":418,"user":{"displayName":"Henry Isaac","userId":"09127918064436985573"}},"outputId":"846e52fd-a81d-426b-c5cf-bc159a1cd78a","id":"j_yS1s1tdrMs"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[TopLogprob(token='business', bytes=[98, 117, 115, 105, 110, 101, 115, 115], logprob=-0.044131428),\n"," TopLogprob(token='Business', bytes=[66, 117, 115, 105, 110, 101, 115, 115], logprob=-3.1586373)]"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":[],"metadata":{"id":"g5HhUk4hm2Rb"},"execution_count":null,"outputs":[]}]}